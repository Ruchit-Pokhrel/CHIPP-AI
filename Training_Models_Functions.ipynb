{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQMLmN6oSTzm",
        "outputId": "b6769564-1d12-4e9d-eb18-df05114f7833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoConfig, DebertaV2Config, DebertaV2ForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Qrpa52fMSeX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name= 'microsoft/deberta-v3-base'"
      ],
      "metadata": {
        "id": "NTBhL0egSkL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(train_path,test_path,val_path):\n",
        "\n",
        "  \"\"\"loads train,test and val dataframes from csv path then converts it to Hugging Face datasets\"\"\"\n",
        "\n",
        "  train_df=pd.read_csv(train_path)\n",
        "  test_df=pd.read_csv(test_path)\n",
        "  val_df=pd.read_csv(val_path)\n",
        "\n",
        "  train_dataset=Dataset.from_pandas(train_df)\n",
        "  test_dataset=Dataset.from_pandas(test_df)\n",
        "  val_dataset=Dataset.from_pandas(val_df)\n",
        "\n",
        "  return train_dataset,test_dataset,val_dataset"
      ],
      "metadata": {
        "id": "5tZ87_ygSsq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "p3I8tSQHTC03",
        "outputId": "4862f012-6932-470b-91c5-a588a392cddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b52578a2b5d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "  return tokenizer(example['review'], truncation=True, padding=\"max_length\", max_length=256)"
      ],
      "metadata": {
        "id": "6TWJmPh2TwvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_datasets(train_dataset, val_dataset, test_dataset, tokenize_function):\n",
        "\n",
        "    \"\"\"Tokenizes the train, validation, and test datasets.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return tokenized_train, tokenized_val, tokenized_test"
      ],
      "metadata": {
        "id": "CTgAlIQRUKnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "dIm_KTfuUeqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ.get('CUDA_VISIBLE_DEVICES')\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "id": "-OayyfbgUhOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")"
      ],
      "metadata": {
        "id": "zUOAiOHnUjaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_memory():\n",
        "    '''\n",
        "    Free GPU memory\n",
        "    '''\n",
        "    # Trigger Python garbage collection\n",
        "    gc.collect()\n",
        "    # Clear PyTorch's CUDA cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "id": "eLKpGhw3Ulno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'mcc': mcc,\n",
        "    }"
      ],
      "metadata": {
        "id": "v9KIwL8DUnJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=256,\n",
        "    learning_rate= 2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    label_smoothing_factor=0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.98,\n",
        "    warmup_ratio=0.1,\n",
        "    run_name=\"model40_finetuning\",\n",
        "    report_to='wandb',\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "p_UOQ-XPUpR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_args, tokenized_train, tokenized_val, compute_metrics):\n",
        "    \"\"\"Loading trainer\n",
        "\n",
        "    \"\"\"\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "UMXpg5opWaW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(trainer, dataset):\n",
        "\n",
        "  results=trainer.evaluate(eval_dataset=dataset)\n",
        "  print(results)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "KeVcPklIYIUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trainer_logs(trainer):\n",
        "    \"\"\"\n",
        "    Extracts training and validation loss from trainer log history and plots them.\n",
        "\n",
        "    Args:\n",
        "        trainer: Hugging Face Trainer object\n",
        "    \"\"\"\n",
        "    logs = trainer.state.log_history\n",
        "\n",
        "    # Extract losses and corresponding steps\n",
        "    train_losses = [entry[\"loss\"] for entry in logs if \"loss\" in entry]\n",
        "    eval_losses = [entry[\"eval_loss\"] for entry in logs if \"eval_loss\" in entry]\n",
        "    steps = [entry[\"step\"] for entry in logs if \"loss\" in entry]\n",
        "    eval_steps = [entry[\"step\"] for entry in logs if \"eval_loss\" in entry]\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(steps, train_losses, label=\"Training Loss\", marker=\"o\")\n",
        "    plt.plot(eval_steps, eval_losses, label=\"Validation Loss\", marker=\"x\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Convert logs to DataFrame\n",
        "    df = pd.DataFrame(logs)\n",
        "\n",
        "    # Plot training loss separately\n",
        "    df_loss = df[df['loss'].notna()]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_loss['step'], df_loss['loss'], label='Training Loss')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot validation loss separately\n",
        "    df_eval = df[df['eval_loss'].notna()]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_eval['step'], df_eval['eval_loss'], label='Validation Loss', color='orange')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BquEnL2lYuAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path='/content/drive/MyDrive/Datasets/Arts_Crafts_and_Sewing_train_1.csv'\n",
        "test_path='/content/drive/MyDrive/Datasets/Arts_Crafts_and_Sewing_test.csv'\n",
        "val_path='/content/drive/MyDrive/Datasets/Arts_Crafts_and_Sewing_val.csv'"
      ],
      "metadata": {
        "id": "IqG0L4yWIPMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,test_dataset,val_dataset=load_datasets(train_path=train_path,test_path=test_path,val_path=val_path)"
      ],
      "metadata": {
        "id": "bXFcAyyZc29P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train, tokenized_val, tokenized_test=tokenize_datasets(train_dataset=train_dataset, val_dataset=val_dataset,\n",
        "                                                                 test_dataset=test_dataset, tokenize_function=tokenize)"
      ],
      "metadata": {
        "id": "gy44JtUZNCjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DebertaV2ForSequenceClassification.from_pretrained(model_name, hidden_dropout_prob=0.1,\n",
        "                                                            attention_probs_dropout_prob=0.1, num_labels=3)"
      ],
      "metadata": {
        "id": "sRVYJJ_UT6LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=train_model(model=model, training_args=training_args,\n",
        "                    tokenized_train=tokenzied_train, tokenized_val=tokenized_val, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "t9a9IdwXUBWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_trainer_logs(trainer=trainer)"
      ],
      "metadata": {
        "id": "M2Aj_T-0ZqfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}